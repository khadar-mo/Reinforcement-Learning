{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTNU1mwGB1ZD"
   },
   "source": [
    "**Dependencies and setup**\n",
    "\n",
    "This can take a minute or so...\n",
    "\n",
    "References -\n",
    "code - https://github.com/quantumiracle/Popular-RL-Algorithms/blob/master/sac_v2.py\n",
    "paper - https://arxiv.org/pdf/1812.05905.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA38jtUgtZsG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
    "!apt update\n",
    "!apt-get install python3-opengl\n",
    "!apt install xvfb -y\n",
    "!pip install 'swig'\n",
    "!pip install 'pyglet==1.5.27'\n",
    "!pip install 'gym[box2d]==0.20.0'\n",
    "!pip install 'pyvirtualdisplay==3.0'\n",
    "\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Normal\n",
    "from IPython.display import clear_output\n",
    "import sys\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "%matplotlib inline\n",
    "\n",
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "plot_interval = 10 # update the plot every N episodes\n",
    "video_every = 100 # videos can take a very long time to render so only do it every N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSWjVV30Qj3f"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch)) # stack for each element\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "    def action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        # Normalize the action\n",
    "        normalized_action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        normalized_action = np.clip(normalized_action, low, high)\n",
    "\n",
    "        return normalized_action\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "\n",
    "        # Reverse the normalization\n",
    "        reversed_action = 2 * (action - low) / (high - low) - 1\n",
    "        reversed_action = np.clip(reversed_action, -1, 1)  # Assuming the original action is normalized between -1 and 1\n",
    "\n",
    "        return reversed_action\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear4 = nn.Linear(hidden_dim, 1)\n",
    "        # weights initialization\n",
    "        self.linear4.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear4.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.linear4.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear4.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1) # the dim 0 is number of samples\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, action_range=1., init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear4 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "        self.action_range = action_range\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "\n",
    "        mean    = (self.mean_linear(x))\n",
    "        # mean    = F.leaky_relu(self.mean_linear(x))\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "\n",
    "        return mean, log_std\n",
    "\n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        '''\n",
    "        generate sampled action with state as input wrt the policy network;\n",
    "        '''\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp() # no clip in evaluation, clip affects gradients flow\n",
    "\n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample(mean.shape)\n",
    "        action_0 = torch.tanh(mean + std*z.to(device)) # TanhNormal distribution as actions; reparameterization trick\n",
    "        action = self.action_range*action_0\n",
    "        log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1. - action_0.pow(2) + epsilon) -  np.log(self.action_range)\n",
    "\n",
    "        log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        return action, log_prob, z, mean, log_std\n",
    "\n",
    "\n",
    "    def get_action(self, state, deterministic):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "\n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample(mean.shape).to(device)\n",
    "        action = self.action_range* torch.tanh(mean + std*z)\n",
    "\n",
    "        action = self.action_range* torch.tanh(mean).detach().cpu().numpy()[0] if deterministic else action.detach().cpu().numpy()[0]\n",
    "        return action\n",
    "\n",
    "\n",
    "    def sample_action(self,):\n",
    "        a=torch.FloatTensor(self.num_actions).uniform_(-1, 1)\n",
    "        return self.action_range*a.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJHtclV_30Re"
   },
   "source": [
    "**Reinforcement learning agent**\n",
    "\n",
    "Replace this with your own agent - I recommend starting with TD3 (lecture 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jXNHP8_U-rn"
   },
   "outputs": [],
   "source": [
    "class SAC_Trainer():\n",
    "    def __init__(self, replay_buffer, hidden_dim, action_range):\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        self.soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim, action_range).to(device)\n",
    "        self.log_alpha = torch.zeros(1, dtype=torch.float32, requires_grad=True, device=device)\n",
    "        print('Soft Q Network (1,2): ', self.soft_q_net1)\n",
    "        print('Policy Network: ', self.policy_net)\n",
    "\n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        self.soft_q_criterion1 = nn.MSELoss()\n",
    "        self.soft_q_criterion2 = nn.MSELoss()\n",
    "\n",
    "        soft_q_lr = 3e-4\n",
    "        policy_lr = 3e-4\n",
    "        alpha_lr  = 3e-4\n",
    "\n",
    "        self.soft_q_optimizer1 = optim.Adam(self.soft_q_net1.parameters(), lr=soft_q_lr)\n",
    "        self.soft_q_optimizer2 = optim.Adam(self.soft_q_net2.parameters(), lr=soft_q_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "\n",
    "\n",
    "    def update(self, batch_size, reward_scale=5., auto_entropy=True, target_entropy=-1, gamma=0.99,soft_tau=1e-2):\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "        # print('sample:', state, action,  reward, done)\n",
    "\n",
    "        state      = torch.FloatTensor(state).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        action     = torch.FloatTensor(action).to(device)\n",
    "        reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)  # reward is single value, unsqueeze() to add one dim to be [reward] at the sample dim;\n",
    "        done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "        predicted_q_value1 = self.soft_q_net1(state, action)\n",
    "        predicted_q_value2 = self.soft_q_net2(state, action)\n",
    "        new_action, log_prob, z, mean, log_std = self.policy_net.evaluate(state)\n",
    "        new_next_action, next_log_prob, _, _, _ = self.policy_net.evaluate(next_state)\n",
    "        reward = reward_scale * (reward - reward.mean(dim=0)) / (reward.std(dim=0) + 1e-6) # normalize with batch mean and std; plus a small number to prevent numerical problem\n",
    "    # Updating alpha wrt entropy\n",
    "        if auto_entropy is True:\n",
    "            alpha_loss = -(self.log_alpha * (log_prob + target_entropy).detach()).mean()\n",
    "            # print('alpha loss: ',alpha_loss)\n",
    "            self.alpha_optimizer.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            self.alpha_optimizer.step()\n",
    "            self.alpha = self.log_alpha.exp()\n",
    "        else:\n",
    "            self.alpha = 1.\n",
    "            alpha_loss = 0\n",
    "\n",
    "    # Training Q Function\n",
    "        target_q_min = torch.min(self.target_soft_q_net1(next_state, new_next_action),self.target_soft_q_net2(next_state, new_next_action)) - self.alpha * next_log_prob\n",
    "        target_q_value = reward + (1 - done) * gamma * target_q_min # if done==1, only reward\n",
    "        q_value_loss1 = self.soft_q_criterion1(predicted_q_value1, target_q_value.detach())  # detach: no gradients for the variable\n",
    "        q_value_loss2 = self.soft_q_criterion2(predicted_q_value2, target_q_value.detach())\n",
    "\n",
    "\n",
    "        self.soft_q_optimizer1.zero_grad()\n",
    "        q_value_loss1.backward()\n",
    "        self.soft_q_optimizer1.step()\n",
    "        self.soft_q_optimizer2.zero_grad()\n",
    "        q_value_loss2.backward()\n",
    "        self.soft_q_optimizer2.step()\n",
    "\n",
    "    # Training Policy Function\n",
    "        predicted_new_q_value = torch.min(self.soft_q_net1(state, new_action),self.soft_q_net2(state, new_action))\n",
    "        policy_loss = (self.alpha * log_prob - predicted_new_q_value).mean()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # print('q loss: ', q_value_loss1, q_value_loss2)\n",
    "        # print('policy loss: ', policy_loss )\n",
    "\n",
    "\n",
    "    # Soft update the target value net\n",
    "        for target_param, param in zip(self.target_soft_q_net1.parameters(), self.soft_q_net1.parameters()):\n",
    "            target_param.data.copy_(  # copy data value into target parameters\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "        for target_param, param in zip(self.target_soft_q_net2.parameters(), self.soft_q_net2.parameters()):\n",
    "            target_param.data.copy_(  # copy data value into target parameters\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "        return predicted_new_q_value.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEv4ZjXmyrHo"
   },
   "source": [
    "**Prepare the environment and wrap it to capture videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xrcek4hxDXl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#env = NormalizedActions(gym.make(\"BipedalWalker-v3\"))\n",
    "env = NormalizedActions(gym.make(\"BipedalWalkerHardcore-v3\"))# only attempt this when your agent has solved BipedalWalker-v3\n",
    "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id % 25 == 0, force=True)\n",
    "\n",
    "replay_buffer_size = 1e6\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "action_range=1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRJkEx9nZo9s"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXTHFPhSo1fM"
   },
   "outputs": [],
   "source": [
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGzS_0VJOpBr"
   },
   "outputs": [],
   "source": [
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUw4h980jfnu",
    "outputId": "79e08ce7-7f3b-4366-eeb9-b128f39f123b"
   },
   "outputs": [],
   "source": [
    "print('The environment has {} observations and the agent can take {} actions'.format(state_dim, action_dim))\n",
    "print('The device is: {}'.format(device))\n",
    "\n",
    "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "rDl6ViIDlVOk",
    "outputId": "3b474842-2e47-474e-a4cd-16e943619d4f"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "#hyperparameters \n",
    "frame_idx   = 0\n",
    "batch_size  = 640\n",
    "update_itr = 1\n",
    "AUTO_ENTROPY=True\n",
    "DETERMINISTIC=False\n",
    "explore_steps = 0\n",
    "hidden_dim = 512\n",
    "target_entropy = -1.*action_dim\n",
    "\n",
    "# logging variables\n",
    "ep_reward = 0\n",
    "reward_list = []\n",
    "plot_data = []\n",
    "track_score = []\n",
    "log_f = open(\"agent-log.txt\",\"w+\")\n",
    "\n",
    "# initialise agent\n",
    "agent = SAC_Trainer(replay_buffer, hidden_dim=hidden_dim, action_range=action_range)\n",
    "max_episodes = 2000\n",
    "max_timesteps = 2000\n",
    "# training procedure:\n",
    "for episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        # select the agent action\n",
    "        if frame_idx > explore_steps:\n",
    "            action = agent.policy_net.get_action(state, deterministic = DETERMINISTIC)\n",
    "        else:\n",
    "            action = agent.policy_net.sample_action()\n",
    "\n",
    "        # take action in environment and get r and s'\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        ep_reward += reward\n",
    "        frame_idx += 1\n",
    "\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            for i in range(update_itr):\n",
    "                _ =agent.update(batch_size, reward_scale=7., auto_entropy=AUTO_ENTROPY, target_entropy=target_entropy)\n",
    "\n",
    "        # stop iterating when the episode finished\n",
    "        if done or t==(max_timesteps-1):\n",
    "            break\n",
    "\n",
    "    # append the episode reward to the reward list\n",
    "    reward_list.append(ep_reward)\n",
    "    #once it has 100 consecative score >300 solution can be considered complete\n",
    "    track_score.append(ep_reward)\n",
    "    #average_score = np.mean(reward_list[-100:])\n",
    "\n",
    "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
    "    log_f.flush()\n",
    "    ep_reward = 0\n",
    "\n",
    "    # prints reward data every so often \n",
    "    if episode % plot_interval == 0:\n",
    "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
    "        reward_list = []\n",
    "        # plt.rcParams['figure.dpi'] = 100\n",
    "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "        plt.xlabel('Episode number')\n",
    "        plt.ylabel('Episode reward')\n",
    "        plt.show()\n",
    "        disp.clear_output(wait=True)\n",
    "    #if len(track_score) >= 100 and all(score > 300 for score in track_score[-100:]):\n",
    "    #    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4fDygoHS0OO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8K89IANpIsc"
   },
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
    "plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Episode reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CadDs057pV2U",
    "outputId": "53511c20-0fa0-476c-d526-500e6b770bd1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attempt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHGenXCsqDf-",
    "outputId": "b0489ff5-d738-4b39-f8b7-3deb97f7c085"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiwdyJFR1BwW"
   },
   "outputs": [],
   "source": [
    "# just some set up code when i was using drive for google colab when ncc was down\n",
    "import shutil\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "\n",
    "drive_path = '/content/drive/My Drive/'\n",
    "source_folder = '/content/video'\n",
    "source_file = '/content/agent-log.txt'\n",
    "\n",
    "destination_folder = '/content/drive/My Drive/YourDestinationFolder4/video'\n",
    "destination_file = '/content/drive/My Drive/YourDestinationFolder4/agent-log.txt'\n",
    "\n",
    "\n",
    "shutil.copytree(source_folder, destination_folder)\n",
    "shutil.copy(source_file, destination_file)\n",
    "\n",
    "print(f'File \"agent-log.txt\" has been copied to {destination_file}')\n",
    "print(f'Folder \"video\" has been copied to {destination_folder}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpY_EjmW1DA2",
    "outputId": "808ca731-95f3-4d28-dfce-8b9833eadbae"
   },
   "outputs": [],
   "source": [
    "average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPUCLdzMrXFr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "newmykernel2",
   "language": "python",
   "name": "newmykernel2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
